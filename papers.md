# Papers

The year column represents publishing year in arxiv and not the year of latest revision. `Supplementary` pdf/docs is mostly not included in the links below. A paper may fall into multiple categories but organized into a general one.

Adding only papers worth implementing, important concepts that can be applied in future with high quality or need to be revisited again with easy to understand, SOTA, close to SOTA results or unique ideas. Not adding paper if concept not understood, too hard or anything meaningful not found.

<!--
|  |  |
| <h3></h3> |  |
-->

## Papers Read

- :triangular_flag_on_post: Represents overall good understanding, easier to understand with diagrams, examples or good explanation simple language.

| Topic | Year |
| --- | --- |
| <h3>Uncategorized</h3> |  |
| [Feature Pyramid Networks for Object Detection (FPN)](https://arxiv.org/abs/1612.03144) | 2016 | 
| [COIN: COmpression with Implicit Neural representations](https://arxiv.org/abs/2103.03123) | 2021 |    
| [MaskGAN: Towards Diverse and Interactive Facial Image Manipulation](https://arxiv.org/abs/1907.11922) | 2019 |   
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | 2017 |  
| [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) | 2020 |    
| [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672) | 2021 |  
| [Deep Image Prior](https://openaccess.thecvf.com/content_cvpr_2018/papers/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.pdf) <br> [Supplementary](https://openaccess.thecvf.com/content_cvpr_2018/Supplemental/2711-supp.pdf) | 2018 |  
| [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) | 2023 |  
| [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685) | 2021 |  
|  |  |
| <h3>Vision Transformers</h3> |  |
| :triangular_flag_on_post: [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE <br> (Vision Transformer, ViT)](https://arxiv.org/abs/2010.11929) | 2020 |  
|  |  |
| <h3>Knowledge Distillation</h3> |  |
| :triangular_flag_on_post: [Distilling the Knowledge in a Neural Network (Knowledge Distillation)](https://arxiv.org/abs/1503.02531) | 2015 |  
| :triangular_flag_on_post: [On the Efficacy of Knowledge Distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.pdf) | 2019 |  |  
|  |  |
| <h3>Object Recognition, Clutering, Verification</h3> |  |
| :triangular_flag_on_post: [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf) | 2015 |  
| :triangular_flag_on_post: [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) | 2015 |  
|  |  |
| <h3>Image Captioning</h3> |  |
| [Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge](https://arxiv.org/abs/1609.06647) | 2016 |  
|  |  |
| <h3>Text to Image Generation</h3> |  |
| [Zero-Shot Text-to-Image Generation (DALL-E)](https://arxiv.org/pdf/2102.12092.pdf) | 2021 |  
|  |  |
| <h3>Multimodal Deep Learning</h3> |  |
| [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/pdf/2103.00020.pdf) | 2021 |  
|  |  |
| <h3>Image Super Resolution</h3> |  |
| [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (SRGAN)](https://arxiv.org/abs/1609.04802) | 2016 |  
|  |  |
| <h3>Image Segmentation</h3> |  |
| [U-Net: Convolutional Networks for Biomedical Image Segmentation (UNet)](https://arxiv.org/abs/1505.04597) | 2015 |  
|  |  |
| <h3>Convolutional Neural Network (CNN) Architectures</h3> |  |
| [ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)](https://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf) | 2012 |  
| [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261) | 2016 |  
| [Going deeper with convolutions (Inception/GoogLeNet)](https://arxiv.org/abs/1409.4842) | 2014 |  
| [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION (VGG)](https://arxiv.org/abs/1409.1556) | 2014 |  
| :triangular_flag_on_post: [Wide Residual Networks (WRN)](https://arxiv.org/abs/1605.07146) | 2016 |  
| :triangular_flag_on_post: [Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/pdf/1512.03385.pdf) | 2015 |  
| :triangular_flag_on_post: [Aggregated Residual Transformations for Deep Neural Networks (ResNeXt)](https://arxiv.org/abs/1611.05431) | 2016 |  
|  |  |
| <h3>Survey/Review Papers</h3> |  |
| [GAN Inversion: A Survey](https://arxiv.org/abs/2101.05278) | 2021 |  
|  |  |
| <h3>Generative Adversarial Network (GAN)</h3> |  |
| [Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661) | 2014 |  
| [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)](https://arxiv.org/abs/1511.06434) | 2015 |  
| [Improved Training of Wasserstein GANs (WGAN-GP)](https://arxiv.org/abs/1704.00028) | 2017 |   
| [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) | 2014 |  
| :triangular_flag_on_post: [Analyzing and Improving the Image Quality of StyleGAN (StyleGAN 2)](https://arxiv.org/pdf/1912.04958.pdf) | 2019 |  
| :triangular_flag_on_post: [Training Generative Adversarial Networks with Limited Data (StyleGAN 2 ADA)](https://arxiv.org/pdf/2006.06676.pdf) | 2020 |  
| :triangular_flag_on_post: [Alias-Free Generative Adversarial Networks (StyleGAN 3)](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf) | 2021 |  
| :triangular_flag_on_post: [PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION (ProGAN)](https://arxiv.org/abs/1710.10196) | 2017 |  
| :triangular_flag_on_post: [A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN)](https://arxiv.org/pdf/1812.04948.pdf) | 2018 |  
|  |  |
| <h3>Image to Image Translation</h3> |  |
| [Image-to-Image Translation with Conditional Adversarial Networks (pix2pix)](https://arxiv.org/abs/1611.07004) | 2016 |  
| [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN)](https://arxiv.org/abs/1703.10593) | 2017 |  
| :triangular_flag_on_post: [Semantic Image Synthesis with Spatially-Adaptive Normalization (GauGAN/SPADE)](https://arxiv.org/abs/1903.07291) | 2019 |
|  |  |
| <h3>Neural Style Transfer (NST)</h3> |  |
| [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) <br> [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) | 2016 <br> 2015 | 
| :triangular_flag_on_post: [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155) <br> :triangular_flag_on_post: [Supplementary](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf) | 2016 |  
|  |  |
| <h3>Language Models</h3> |  |
| [Language Models are Unsupervised Multitask Learners (GPT-2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | 2019 |

<br>

## Reading List

| Topic | Year | 
| --- | --- | 
| [SELF-ATTENTION DOES NOT NEED O(n^2) MEMORY](https://arxiv.org/pdf/2112.05682v2.pdf) |  |  
| [Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/pdf/2111.07624v1.pdf) |  |
| [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf) | 2023 |

<br>

# Summary

### Language Models

### 2018

### [Improving Language Understanding by Generative Pre-Training (GPT)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

<details>

 **Basics**
 - For `multi-task` Natural Language Understanding (NLU) objectives such as question answering, semantic similarity, document classification etc.
 - Some of these tasks are part of [General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) multi-task benchmark.
 - Supervised learning suffer from lack of large annotated dataset and quality. Learning from raw text removes dependence of on supervised only methods.

 **GPT**
 - GPT (Generative Pre-Training) is a `semi-supervised` approach with `unsupervised pre-training` and `supervised fine-tuning`. 
 - `Two stages`, Generative Pre-Training on unlabeled data, Discriminative Finetuning for each specific task with task aware input transformation and minimal model architecture change.
 - GPT aquires useful linguistic knowledge for `downstream tasks` and outperforms specifically crafted task specific models.
 - Goal is to `learn universal representation` to apply to wide range of tasks with little adaptation.
 -  Input text is processed as single contiguous sequence of tokens.
 - `Training` requires `large corpus of unlabeled data` and `manually annotated data` for each target task.

 **Model**
 - `Transformer` is used for model architecture due ability `handle long-term dependencies` in text.
 - Multi-layer `transformer decoder` is used for language modeling.
 - Models is multiple layer `decoder only transformer` with masked self attention heads.
 - `Learned positional embedding` is used instead of sinusoidal in original transformers.

 **Unsupervised Pre-training**
 - `Pre-training` acts as `regularizer` providing `better generalization` in deep neural nets. 
 - `Unsupervised pretraining` goal is to find `good initialization point` instead of modifying supervised objective.
 - Unsupervised pretraining objective based on `context window of tokens` predicts likelihood the next token.

 **Supervised Fine-tuning**
 - Uses `labeled dataset` for supervised task. Input sequence of tokens `x1, x2, ..., xN` has output label `y`.
 - An `additional linear ouput layer` is added after final layer of transformer to predict `y` for given task.
 - Uses `label prediction objective` and additionally `language modeling as auxiliary objective` (loss) of unsupervised pre-training for supervised-finetuning.
 - `Extra parameters` added to unsupervised pre-trained model is final linear layer weights $W_y$ and embedding for delimiter tokens. 

 **Input Transformation**
 - `Byte Pair Encoding (BPE)` used for sub-word tokenization.
 - Instead of using task specific architectures, the inputs are `converted to sequence of tokens` that the pretrained model can process.
 - Includes `start and end tokens` for each input to pretrained model, `<s>`, `<e>`.
 - For `text entailment`, premise `p` and hypothesis `h` token sequence inlcude delimiter token `$` between them.
 - For `document QA` (Question Answering) and common sense reasoning include document `z`, a question `q`, a set of answers $a_k$. Document context, question and each individual answer is concatenated with delimiter token in between to produce input data to the pretrained model ($z$, $q$, $\\$$, $a_1$), ($z$, $q$, $\\$$, $a_2$), ..., ($z$, $q$, $\\$$, $a_K$). Softmax layer is used to produce output distribution over all possible answers. 
 - For `sentence similarity` with two sentence sequence of tokens `s1`, `s2` there is no ordering. Start, end tokens are added with delimiter between the sentences to produce output. Also the sentences are swapped to again produce output both which is concatenated before feeding to linear layer.

 **Datasets**
 - Natural Language inference (NLI): SNLI, MNLI, QNLI.
 - Question Answering: SQuaD.
 - Semantic Similarity: Quora Question Pairs (QQP), Semantic Textual Similarity benchmark (STS-B), Microsoft Paraphrase corpus (MRPC).

</details>

### [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (BERT)](https://arxiv.org/abs/1810.04805)

<details>  

 **Applying Pre-trained Language Representations to Downstream tasks**
 - `Two strategies` for applying pre-trained language representations downstream tasks. They are `fine-tuning` and `feature-based`.
 - Feature-based representations are applied as `additional features` to task-specific architectures.
 - Fine-tuning approaches like `GPT` introduces `minimal task specific parameters`. It is trained by downstream tasks by `fine-tuning all pre-trained parameters`.

 **Limitations of existing Unidirectional Language Models**
 - Standard language models are `unidirectional`. `GPT` uses `left-to-right` architecture, where can `only attend to previous tokens` in self attention layer by masking future tokens.
 - This can be `harmful` when fine-tuning on `token-level tasks like Question Answering`, which requires context incorporated from both directions.

 **BERT**
 - BERT (Bidirectional Encoder Representations from Transformers) is a `language representation model` introduced in the paper.
 - Similar to GPT, BERT has pre-training and fine-tuning stage. Each downstream task has separate finetuned model even if they are initialized with same pre-trained model parameters.
 - In contrast to GPT, BERT is a `transformer encoder model`.
 - Designed to `pretrain` deep bidirectional representations from `unlabeled data` by `jointly conditioning on left and right context` in all layers.
 - `An additional output layer` is added to create `SOTA (state-of-the-art)` models that performs well on sentence-level and token-level tasks. Tasks include QA, language inference etc.
 - `Alleviates constraints` of unidirectional models (e.g. GPT) by introducing `Masked Language Model (MLM)` pre-training task. 
 - Also uses `Next Sentence Prediction (NSP)` task to jointly pretrain on text-pair representations.
 - Similar to GPT, during finetuning `all parameters are fine-tuned`.

 **Masked Language Model (MLM)**
 - MLM `randomly masks some input tokens`, and the objective is to `predict original vocabulary id` based only on context. 
 - MLM enables representations to `fuse left and right context`.

 **Special Tokens**
 - `[CLS]` token added in front of every training example. Final state corresponding to this token is used as aggregate representation for classification tasks.
 - `[SEP]` special separator token for separating sentence pairs. It can be used for separating question and answers. 
 
 **Input and Ouput Representations**
 - BERT input representation can unambiguously handle both single and paired sentence `(<question, answer>)` in one token sequence.
 - Here, a sentence is considered as `arbitrary span of contiguous text`, rather than liguistic sentence.
 - Sequence here is referred as `input token sequence` to BERT which can be a `single sentence or pair of sentences packed together`.
 - `Learned positional embedding` is also added to each tokens for indicating whether it belongs to sentence A or B.
 
 </details>

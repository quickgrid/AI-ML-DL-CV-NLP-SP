# Knowledge Distillation Family

### Papers

| Topic | Year | Conference |
| --- | --- | --- |
| [Distilling the Knowledge in a Neural Network (Knowledge Distillation)](https://arxiv.org/abs/1503.02531) | 2015 |  |

# Distilling the Knowledge in a Neural Network

## Knowledge Distillation Details

<img src="figures/knowledge-distillation/knowledge_distillation_1.png" width=80% height=80%>

## Training Objectives

<img src="figures/knowledge-distillation/knowledge_distillation_2.png" width=80% height=80%>

## Experiment Details

<img src="figures/knowledge-distillation/knowledge_distillation_3.png" width=80% height=80%>

# On the Efficacy of Knowledge Distillation

Bigger models with more accuracy does not mean they are good teachers.


# Does Knowledge Distillation Really Work?

